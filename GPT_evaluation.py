import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import httpx
import backoff
import argparse
import openai
from openai import OpenAI
import json

def generate(prompt, content):
    client = OpenAI(api_key=api_key)
    message = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": content},
    ]
    global total_length
    total_length += len(str(message))

    try:
        response = client.chat.completions.create(
            messages=message,
            model="gpt-4o",
            # temperature=0.2
        )
        return response
    except Exception as e:
        print(f"Error generating completion: {e}")
        return None

def open_question_evaluate(content):
    prompt = f"""You will be provided with a question, a ground truth answer, and a model-generated result related to a pathology case. Your task is to evaluate the correctness of the model's predicted results.
    Instructions for evaluation:
    1.	If the model's result and the ground truth answer contain similar or equivalent information, it is correct, even if the wording or structure differs.
    2.	If the model's result is broader but includes the ground truth answer or its key elements, it should still be considered correct.
    3.	If part of the model's result matches part of the key information from the ground truth answer, the result is partially correct.
    4.	If the model's result contradicts the ground truth answer, it is wrong. 
    5.  If the model's result and the ground truth answer do not overlap, but the result could be an answer based on the question, it is uncertain.
    6.	For yes/no questions, focus only on the correctness of the yes/no response itself.
    In the response, provide only a number:
    -	2 if the model's result is correct,
    -	1 if it is partially correct,
    -	-1 if it is incorrect,
    -   0 if it is uncertain.
    Ensure your assessment reflects the clinical context and evaluates the accuracy of the provided content.
    """
    response = generate(prompt, content)
    res = eval(response.choices[0].message.content.strip())
    return res

def description_evaluation(content):
    prompt = f"""Prompt for Evaluating Pathology Slide Description Generation Models

    Role: Senior Pathologist

    Task: You will be provided with an answer (ground truth description of a pathology slide) and a model-generated description. Your role is to evaluate the quality and accuracy of the model-generated description compared to the ground truth.
    Content:
    - Answers (Ground Truth): [Ground truth description of the pathology slide]
    - Results (Model-generated description): [Description generated by the model]
    Evaluation Criteria: You will evaluate the generated description based on the following aspects:
    1. Accuracy: How well does the description match the ground truth? Does it accurately reflect the histological features and diagnosis presented in the pathology slide?
    2. Completeness: Does the description cover all essential aspects and details mentioned in the ground truth?
    3. Clarity: How clear and understandable is the description? Is the language precise and appropriate for a pathological context?
    4. Relevance: Are the described features and details directly relevant to the diagnosis indicated in the ground truth?
    5. Conciseness: Is the description concise yet informative, without unnecessary details or redundancies?
    Scoring: Provide an overall score between 0-10 for the model-generated description:
    - 0-3: Poor - Major inaccuracies and multiple important details missing.
    - 4-6: Fair - Some relevant information is present, but with notable inaccuracies or omissions.
    - 7-8: Good - Mostly accurate and complete, with minor discrepancies.
    - 9-10: Excellent - Nearly perfect match with the ground truth, accurately and clearly described.
    Task Execution:
    - Rewrite the Description: Based on the ground truth, write your own version of what an ideal description should be.
    - Relative Scoring: Compare the model-generated description to your rewritten description and score it relatively.
    - Absolute Scoring: Directly score the model-generated description against the ground truth for an absolute evaluation.
    In the response, provide only two numbers, as json format, {{"relative": score, "absolute": score}}.
    """
    response = generate(prompt, content)
    res = response.choices[0].message.content.strip()
    res = json.loads(res)
    return res


import os
import argparse
import pandas as pd
from tqdm import tqdm

# 获取 API Key
api_key = os.getenv("OPENAI_API_KEY")

# 配置命令行参数
parser = argparse.ArgumentParser(description="处理数据文件名")
parser.add_argument('--type', type=str, default='open', help='类型：open 处理开放式问题，desc 处理描述性问题')
parser.add_argument('--filename', type=str, required=True, help='数据文件名')
parser.add_argument('--suffix', type=str, required=True, help='数据文件名')

args = parser.parse_args()
data_name = args.filename

# 读取数据文件
try:
    open_file = pd.read_csv(data_name)
except Exception as e:
    raise FileNotFoundError(f"无法读取文件 {data_name}，请检查路径和文件格式。错误信息: {e}")

# 初始化变量
if args.type == "open":
    open_file["LLM_result"] = None
    correct, partial, wrong, uncertain = 0, 0, 0, 0

    print("开始处理开放式问题...")
    for i in tqdm(range(len(open_file))):
        content = str(open_file.iloc[i][["question", "answer", "prediction"]].to_dict())
        try:
            res = open_question_evaluate(content)
            res = int(res)  # 确保结果是整数

            if res == 2:
                correct += 1
                open_file.loc[i, "LLM_result"] = 2
            elif res == 1:
                partial += 1
                open_file.loc[i, "LLM_result"] = 1
            elif res == 0:
                wrong += 1
                open_file.loc[i, "LLM_result"] = -1
            elif res == -1:
                uncertain += 1
                open_file.loc[i, "LLM_result"] = 0
            else:
                print(f"未知结果: {res}")
        except Exception as e:
            print(f"处理第 {i} 行时出错: {e}")
            continue

    # 输出结果统计
    print(f"Correct: {correct}, Partial: {partial}, Wrong: {wrong}, Uncertain: {uncertain}")
    accuracy = (correct + partial) / (correct + partial + wrong) if (correct + partial + wrong) > 0 else 0
    print(f"Accuracy: {accuracy:.2f}")

    # 保存结果
    name_root = ".".join(data_name.split(".")[:-1])
    output_file = f"{name_root}_open_test_accuracy.csv"
    open_file.to_csv(output_file, index=False)
    print(f"处理结果已保存到: {output_file}")

elif args.type == "desc":
    open_file["LLM_score_rela"] = None
    open_file["LLM_score_abs"] = None
    relative_score_list = []
    absolute_score_list = []

    print("开始处理描述性问题...")
    for i in tqdm(range(len(open_file))):
        content = str(open_file.iloc[i][["answers", "results"]].to_dict())
        try:
            res = description_evaluation(content)
            open_file.loc[i, "LLM_score_rela"] = res["relative"]
            open_file.loc[i, "LLM_score_abs"] = res["absolute"]

            relative_score_list.append(res["relative"])
            absolute_score_list.append(res["absolute"])
        except Exception as e:
            print(f"处理第 {i} 行时出错: {e}")
            continue

    # 计算平均分
    if relative_score_list and absolute_score_list:
        avg_relative = sum(relative_score_list) / len(relative_score_list)
        avg_absolute = sum(absolute_score_list) / len(absolute_score_list)
        print(f"Average Relative Score: {avg_relative:.2f}")
        print(f"Average Absolute Score: {avg_absolute:.2f}")
    else:
        print("未能计算平均分，可能没有有效结果。")

    # 保存结果
    name_root = ".".join(data_name.split(".")[:-1])
    output_file = f"{name_root}{args.suffix}.csv"
    open_file.to_csv(output_file, index=False)
    print(f"处理结果已保存到: {output_file}")

else:
    raise NotImplementedError(f"处理类型 '{args.type}' 未实现！")

# print("total cost:",total_length/1e6 * 0.15 *7.1)

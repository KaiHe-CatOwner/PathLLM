  0%|                                                                                          | 0/2600 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/run.py", line 173, in <module>
    trainer.train()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/transformers/trainer.py", line 2118, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/transformers/trainer.py", line 3036, in training_step
    loss = self.compute_loss(model, inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/utils/my_trainer.py", line 455, in compute_loss
    outputs = model(**inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
    loss = self.module(*inputs, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/model/my_model.py", line 144, in forward
    fusion_embs = self.get_fusion_embedding(input_ids, image)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/model/my_model.py", line 131, in get_fusion_embedding
    mapped_image_embs = self.fusion_layer_E(mapped_image_embs) # shape (bz, 4096)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 573, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 581, in _sa_block
    x = self.self_attn(x, x, x,
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1189, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/functional.py", line 5140, in multi_head_attention_forward
    is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/functional.py", line 4977, in _mha_shape_check
    raise AssertionError(
AssertionError: query should be unbatched 2D or batched 3D tensor but received 1-D query tensor
Traceback (most recent call last):
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/run.py", line 173, in <module>
    trainer.train()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/transformers/trainer.py", line 2118, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/transformers/trainer.py", line 3036, in training_step
    loss = self.compute_loss(model, inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/utils/my_trainer.py", line 455, in compute_loss
    outputs = model(**inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
    loss = self.module(*inputs, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/model/my_model.py", line 144, in forward
    fusion_embs = self.get_fusion_embedding(input_ids, image)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/model/my_model.py", line 131, in get_fusion_embedding
    mapped_image_embs = self.fusion_layer_E(mapped_image_embs) # shape (bz, 4096)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 573, in forward
    x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 581, in _sa_block
    x = self.self_attn(x, x, x,
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1189, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/functional.py", line 5140, in multi_head_attention_forward
    is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmGZY/lib/python3.10/site-packages/torch/nn/functional.py", line 4977, in _mha_shape_check
    raise AssertionError(
AssertionError: query should be unbatched 2D or batched 3D tensor but received 1-D query tensor
torch.Size([8, 512])

  0%|                                                                                       | 0/11 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/run.py", line 179, in <module>
    trainer.train(resume_from_checkpoint=script_args.resume_from_checkpoint)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/transformers/trainer.py", line 2118, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/transformers/trainer.py", line 3045, in training_step
    self.accelerator.backward(loss)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2007, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
    self.engine.step()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2169, in step
    self._take_model_step(lr_kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2075, in _take_model_step
    self.optimizer.step()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1871, in step
    fp32_partition.to(get_accelerator().current_device_name()).data)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.86 GiB (GPU 0; 39.39 GiB total capacity; 14.69 GiB already allocated; 23.25 GiB free; 14.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/run.py", line 179, in <module>
    trainer.train(resume_from_checkpoint=script_args.resume_from_checkpoint)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/transformers/trainer.py", line 2118, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/transformers/trainer.py", line 3045, in training_step
    self.accelerator.backward(loss)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/accelerate/accelerator.py", line 2007, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 175, in backward
    self.engine.step()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2169, in step
    self._take_model_step(lr_kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2075, in _take_model_step
    self.optimizer.step()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1871, in step
    fp32_partition.to(get_accelerator().current_device_name()).data)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 27.86 GiB (GPU 0; 39.39 GiB total capacity; 14.69 GiB already allocated; 23.25 GiB free; 14.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
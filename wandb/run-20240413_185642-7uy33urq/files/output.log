
  0%|                                                                               | 1/2600 [01:04<46:47:37, 64.82s/it]
  6%|█████▎                                                                              | 3/47 [00:00<00:08,  5.12it/s]
  File "run.py", line 173, in <module>                                                   | 7/47 [00:01<00:10,  3.72it/s]
    trainer.train()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/transformers/trainer.py", line 2193, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/transformers/trainer.py", line 2577, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/transformers/trainer.py", line 3365, in evaluate
    output = eval_loop(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/transformers/trainer.py", line 3544, in evaluation_loop
    for step, inputs in enumerate(dataloader):
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/accelerate/data_loader.py", line 462, in __iter__
    next_batch = next(dataloader_iter)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 678, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/utils/data_collator.py", line 18, in __call__
    return self.torch_call(features)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/utils/data_collator.py", line 45, in torch_call
    temp_list.append(self.image_processor(d["image"]))
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/torchvision/transforms/transforms.py", line 95, in __call__
    img = t(img)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/torchvision/transforms/transforms.py", line 361, in forward
    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/torchvision/transforms/functional.py", line 490, in resize
    return F_pil.resize(img, size=output_size, interpolation=pil_interpolation)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/torchvision/transforms/_functional_pil.py", line 250, in resize
    return img.resize(tuple(size[::-1]), interpolation)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/pathllmHK/lib/python3.8/site-packages/PIL/Image.py", line 2222, in resize
    return self._new(self.im.resize(size, resample, box))
KeyboardInterrupt
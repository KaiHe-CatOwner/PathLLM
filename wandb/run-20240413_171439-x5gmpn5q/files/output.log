
  0%|                                                                                          | 0/5250 [00:00<?, ?it/s]Traceback (most recent call last):
  File "run.py", line 173, in <module>
    trainer.train()
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/transformers/trainer.py", line 2118, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/transformers/trainer.py", line 3036, in training_step
    loss = self.compute_loss(model, inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/utils/my_trainer.py", line 455, in compute_loss
    outputs = model(**inputs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
    loss = self.module(*inputs, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/model/my_model.py", line 144, in forward
    fusion_embs = self.get_fusion_embedding(input_ids, image)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/PathVLM/source/PathLLM/model/my_model.py", line 127, in get_fusion_embedding
    image_embs = self.vision_encoder.encode_image(image, normalize=False) # no proj_contrast=False for clip
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/open_clip/model.py", line 266, in encode_image
    features = self.visual(image)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/open_clip/transformer.py", line 503, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/bask/projects/p/phwq4930-gbm/Zeyu/pyvenv/medfla2/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt